# DeepFake Detection System

## Advanced AI-Generated Image Detection Using EfficientNetB7 with Attention Mechanism

---

## üîó Quick Access

### üìì Master's Thesis Project
[![GitHub Project](https://img.shields.io/badge/GitHub-Master's%20Project-black?style=for-the-badge&logo=github)](https://github.com/CemRoot/Master-Uni-Project)
> **Notebook kodlarƒ±, Flask app ve demo g√∂rselleri i√ßin yukarƒ±daki butona tƒ±klayƒ±nƒ±z** | *Click above for notebooks, Flask app, and demo images*

### ü§ó Pre-trained Models
[![Download Models](https://img.shields.io/badge/ü§ó%20Models-Download-yellow?style=for-the-badge&logo=huggingface)](https://huggingface.co/CemRoot/deepfake-detection-model)
>  *Click above to access pre-trained models*

### üöÄ Live Demos
[![Gradio Demo](https://img.shields.io/badge/ü§ó%20Gradio-Live%20Demo-blue?style=for-the-badge)](https://huggingface.co/spaces/CemRoot/deepfake-detection-aics2025)
[![Streamlit Demo](https://img.shields.io/badge/Streamlit-Live%20App-FF4B4B?style=for-the-badge&logo=streamlit)](https://your-app-url.streamlit.app)

---

## üè∑Ô∏è Badges

[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)
[![Python 3.10](https://img.shields.io/badge/python-3.10-blue.svg)](https://www.python.org/downloads/)
[![TensorFlow 2.15](https://img.shields.io/badge/TensorFlow-2.15-FF6F00.svg)](https://www.tensorflow.org/)

**Presented at the 33rd Irish Conference on Artificial Intelligence and Cognitive Science (AICS 2025)**

Developed by **Emin Cem Koyluoglu**

---

## Abstract

The proliferation of generative artificial intelligence models has enabled the creation of highly realistic synthetic images that are increasingly difficult to distinguish from authentic photographs. This poses significant challenges for digital media integrity, authentication systems, and trust in visual content. This research presents a deep learning-based detection system that addresses this critical challenge through a novel architectural approach.

Our system combines the EfficientNetB7 convolutional neural network architecture with a custom-designed attention mechanism to achieve robust detection of AI-generated imagery. The model has been trained on large-scale datasets encompassing both authentic photographs and synthetic images generated by state-of-the-art generative models, including Generative Adversarial Networks (GANs) and diffusion-based models (Stable Diffusion, DALL-E, Midjourney).

This repository contains the complete implementation deployed as an interactive web application, enabling real-time deepfake detection with comprehensive analysis and visualization capabilities suitable for academic demonstrations and practical applications.

---

## Overview

This system provides binary classification of input images as either **authentic** (genuine photographs) or **synthetic** (AI-generated). The detection pipeline leverages state-of-the-art deep learning techniques, specifically combining:

- **EfficientNetB7**: A highly efficient convolutional neural network architecture that serves as the feature extraction backbone
- **Custom Attention Mechanism**: Enhances discriminative feature learning by focusing on forensically relevant image regions
- **Robust Preprocessing Pipeline**: Ensures consistency with training methodology to maximize detection accuracy

The system achieves high classification accuracy while maintaining computational efficiency suitable for deployment on standard hardware configurations.

## Key Features

### Technical Capabilities
- **High-Performance Detection**: Trained on extensive datasets comprising authentic and synthetic images from multiple generative model families
- **Fast Inference Time**: Classification results delivered within 2-3 seconds on standard CPU architecture
- **Comprehensive Analysis**: Probabilistic confidence scores for both classification categories with detailed diagnostic information
- **Multi-Format Support**: Compatible with JPG, JPEG, and PNG image formats
- **Robust Architecture**: Resilient to common image transformations including compression, resizing, and format conversion

### User Experience
- **Interactive Web Interface**: Modern, intuitive interface built with Streamlit framework
- **Real-Time Processing**: Immediate feedback with visual confidence indicators
- **Cloud Deployment**: Browser-based access requiring no local installation
- **Cross-Platform Compatibility**: Accessible from any device with web browser capability
- **Professional Visualization**: Academic-grade result presentation suitable for conference demonstrations

## Technology Stack

### Deep Learning Framework
- **TensorFlow 2.15**: Primary deep learning framework providing computational backend
- **Keras API**: High-level neural network interface for model construction
- **EfficientNetB7**: State-of-the-art convolutional neural network architecture (backbone)
- **Custom Attention Module**: Novel attention mechanism designed for enhanced discriminative feature learning
- **NumPy**: Numerical computing library for tensor operations

### Image Processing Pipeline
- **OpenCV (cv2)**: Computer vision library for image preprocessing and transformations
- **Pillow (PIL)**: Python Imaging Library for format handling and I/O operations
- **TensorFlow Image Processing**: Native preprocessing utilities for neural network input preparation

### Web Application Framework
- **Streamlit**: Modern Python framework for interactive web application development
- **Custom CSS/HTML**: Professional styling for academic presentation quality
- **Responsive Design**: Optimized layout for various screen sizes and demonstration scenarios

### Deployment Infrastructure
- **Streamlit Community Cloud**: Cloud hosting platform for production deployment
- **Hugging Face Model Hub**: Model repository and versioning system (CemRoot/deepfake-detection-model)
- **GitHub**: Version control system and collaborative development platform
- **Python 3.10+**: Programming language and runtime environment

## Model Architecture

### Network Design

The detection system employs a hierarchical deep learning architecture that combines transfer learning with custom components optimized for synthetic image detection. The complete pipeline is illustrated below:

```
INPUT LAYER
    ‚îî‚îÄ‚îÄ Image Tensor (128 √ó 128 √ó 3)
         RGB color space, normalized pixel values

FEATURE EXTRACTION BACKBONE
    ‚îî‚îÄ‚îÄ EfficientNetB7 (Pre-trained on ImageNet)
         ‚îú‚îÄ‚îÄ Compound Scaling: Depth, Width, Resolution
         ‚îú‚îÄ‚îÄ Mobile Inverted Bottleneck Convolution (MBConv)
         ‚îú‚îÄ‚îÄ Squeeze-and-Excitation Optimization
         ‚îî‚îÄ‚îÄ Output: High-dimensional feature maps

NORMALIZATION LAYER
    ‚îî‚îÄ‚îÄ Batch Normalization
         Stabilizes feature distributions

ATTENTION MECHANISM (Custom Design)
    ‚îî‚îÄ‚îÄ Spatial Attention Module
         ‚îú‚îÄ‚îÄ Conv2D (256 filters, 1√ó1) + ReLU + Dropout(0.5)
         ‚îú‚îÄ‚îÄ Conv2D (128 filters, 1√ó1) + ReLU
         ‚îú‚îÄ‚îÄ Conv2D (128 filters, 1√ó1) + ReLU
         ‚îú‚îÄ‚îÄ Conv2D (1 filter, 1√ó1) + Sigmoid ‚Üí Attention Maps
         ‚îú‚îÄ‚îÄ Element-wise Multiplication (Features √ó Attention)
         ‚îî‚îÄ‚îÄ Global Average Pooling ‚Üí Weighted Feature Vector

CLASSIFICATION HEAD
    ‚îî‚îÄ‚îÄ Fully Connected Layers
         ‚îú‚îÄ‚îÄ Dropout (0.5) - Regularization
         ‚îú‚îÄ‚îÄ Dense (64 units) + ReLU + L2(Œª=0.00001)
         ‚îú‚îÄ‚îÄ Dropout (0.25) - Regularization
         ‚îî‚îÄ‚îÄ Dense (2 units) + Softmax

OUTPUT LAYER
    ‚îî‚îÄ‚îÄ Probability Distribution: [P(Fake), P(Real)]
```

### Architectural Innovations

1. **EfficientNetB7 Backbone**: Provides robust feature extraction through compound scaling optimization, balancing network depth, width, and input resolution for maximum efficiency.

2. **Custom Attention Mechanism**: Unlike standard attention modules, our design incorporates:
   - Progressive channel reduction (256‚Üí128‚Üí128‚Üí1) for computational efficiency
   - Spatial attention maps highlighting forensically relevant image regions
   - Weighted global average pooling for attention-modulated feature aggregation

3. **Regularization Strategy**: Multi-level dropout (0.5, 0.25) and L2 weight regularization prevent overfitting and enhance generalization to unseen generative models.

4. **Compact Input Resolution**: 128√ó128 pixel input enables fast inference while maintaining sufficient spatial resolution for artifact detection.

### Model Specifications

- **Total Parameters**: ~66M (includes EfficientNetB7 backbone)
- **Trainable Parameters**: ~2M (attention module + classification head)
- **Input Dimensions**: 128 √ó 128 √ó 3 (Height √ó Width √ó Channels)
- **Output Classes**: 2 (Binary classification: Fake/Real)
- **Model Size**: ~780 MB (H5 format)
- **Framework**: TensorFlow 2.15 / Keras

## Detection Capabilities

### Supported Generative Models

The system has been trained to detect synthetic images produced by various state-of-the-art generative architectures:

#### Generative Adversarial Networks (GANs)
- **StyleGAN** (v1, v2, v3): High-resolution face and image synthesis
- **ProGAN**: Progressive growing generative networks
- **BigGAN**: Large-scale image generation with class conditioning
- **CycleGAN**: Unpaired image-to-image translation
- **StarGAN**: Multi-domain image translation
- **Other GAN Variants**: Including DCGAN, WGAN, and derivative architectures

#### Diffusion-Based Models
- **Stable Diffusion**: Text-to-image generation with latent diffusion
- **DALL-E** (2, 3): OpenAI's multimodal generative models
- **Midjourney**: Artistic image generation platform
- **Imagen**: Google's text-to-image diffusion model
- **Other Diffusion Models**: Including DDPM, DDIM, and score-based generative models

#### Additional Generative Techniques
- **VAE-based Generators**: Variational autoencoder architectures
- **Autoregressive Models**: PixelCNN and related approaches
- **Neural Style Transfer**: Artistic style application systems
- **Image-to-Image Translation**: Pix2Pix and similar frameworks

### Robustness Characteristics

The detection system maintains reliable performance under common image transformations:

- **Compression Artifacts**: JPEG compression at various quality levels
- **Resolution Changes**: Downsampling and upsampling operations
- **Format Conversion**: Conversion between image formats (PNG ‚Üî JPEG)
- **Mild Post-Processing**: Basic filtering, contrast adjustment, and color correction
- **Partial Cropping**: Detection on cropped image regions

## Installation and Deployment

### System Requirements

#### Hardware Requirements
- **Processor**: Modern multi-core CPU (Intel i5/AMD Ryzen 5 or better recommended)
- **Memory**: Minimum 4 GB RAM (8 GB recommended for optimal performance)
- **Storage**: At least 2 GB free disk space for model and dependencies
- **Network**: Internet connection required for initial model download

#### Software Requirements
- **Operating System**: Linux, macOS, or Windows 10/11
- **Python**: Version 3.10 or higher
- **Package Manager**: pip (latest version recommended)
- **Optional**: CUDA-compatible GPU for accelerated inference (not required)

### Local Installation

For researchers and developers who wish to run the system locally:

#### Step 1: Repository Setup
```bash
# Clone the repository from GitHub
git clone https://github.com/CemRoot/deepfake-detection-streamlit.git

# Navigate to the project directory
cd deepfake-detection-streamlit
```

#### Step 2: Dependency Installation
```bash
# Install required Python packages
pip install -r requirements.txt

# Alternative: Use virtual environment (recommended)
python -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate
pip install -r requirements.txt
```

#### Step 3: Application Launch
```bash
# Start the Streamlit web application
streamlit run app.py
```

#### Step 4: Access Interface
The application will automatically launch in your default web browser at:
```
http://localhost:8501
```

If automatic launch fails, manually navigate to the URL above.

### Cloud Deployment (Streamlit Community Cloud)

For production deployment and conference demonstrations:

#### Step 1: Repository Preparation
```bash
# Initialize Git repository (if not already done)
git init

# Add all project files
git add .

# Create initial commit
git commit -m "Initial commit: DeepFake Detection System for AICS 2025"

# Set main branch
git branch -M main

# Add remote repository
git remote add origin https://github.com/CemRoot/deepfake-detection-streamlit.git

# Push to GitHub
git push -u origin main
```

#### Step 2: Streamlit Cloud Configuration

1. Navigate to [share.streamlit.io](https://share.streamlit.io/)
2. Authenticate with your GitHub account
3. Click **"New app"** button
4. Configure deployment settings:
   - **Repository**: `CemRoot/deepfake-detection-streamlit`
   - **Branch**: `main`
   - **Main file path**: `app.py`
5. Click **"Deploy"** to initiate deployment

The application will be accessible at: `https://your-app-name.streamlit.app`

#### Step 3: Model Accessibility

The trained model is automatically downloaded from Hugging Face Model Hub during first launch:
- **Repository**: `CemRoot/deepfake-detection-model`
- **Model File**: `best_model_effatt.h5` (~780 MB)
- **Caching**: Model is cached for subsequent requests, ensuring fast loading times

---

## Hugging Face Hub Integration

This project provides comprehensive tools for uploading and managing the model on Hugging Face Model Hub, making it easily accessible for research and deployment.

### Features

- ‚úÖ **Automated Upload**: Script to upload model with all metadata
- ‚úÖ **Professional Model Card**: Comprehensive documentation following HF best practices
- ‚úÖ **Model Configuration**: Detailed JSON configuration with all specifications
- ‚úÖ **Inference Examples**: Ready-to-use test scripts for single and batch processing
- ‚úÖ **Git LFS Support**: Proper handling of large model files
- ‚úÖ **Versioning**: Support for model updates with meaningful commit messages

### Quick Start

#### Upload Model to Hugging Face Hub

```bash
# Navigate to the huggingface directory
cd huggingface

# Install dependencies
pip install -r requirements.txt

# Set your Hugging Face token
export HF_TOKEN="your_hugging_face_token"

# Upload model
python upload_model.py \
    --model_path /path/to/your/best_model_effatt.h5 \
    --repo_id CemRoot/deepfake-detection-model
```

#### Test Model from Hub

```bash
# Test single image
python test_inference.py --image test_image.jpg

# Batch processing
python test_inference.py --batch images_folder/
```

### What Gets Uploaded?

When you run the upload script, the following files are automatically uploaded to Hugging Face Hub:

1. **Model File** (`best_model_effatt.h5`) - The trained model weights (~780 MB)
2. **Model Card** (`README.md`) - Comprehensive documentation with:
   - Model description and architecture
   - Supported generative models
   - Usage examples and code snippets
   - Performance metrics
   - Ethical considerations
   - Citation information
3. **Configuration** (`config.json`) - Model metadata including:
   - Architecture specifications
   - Input/output details
   - Preprocessing requirements
   - Performance benchmarks
4. **Inference Example** (`inference_example.py`) - Ready-to-use code for model inference
5. **Git Attributes** (`.gitattributes`) - Proper Git LFS configuration

### Documentation

For detailed instructions (in Turkish), see:
- **[huggingface/README.md](huggingface/README.md)** - Quick start guide
- **[huggingface/HUGGINGFACE_GUIDE.md](huggingface/HUGGINGFACE_GUIDE.md)** - Comprehensive guide covering:
  - Token setup and authentication
  - Upload process and parameters
  - Testing and validation
  - Model updates and versioning
  - Best practices
  - Troubleshooting

### Available Scripts

| Script | Purpose | Example Usage |
|--------|---------|---------------|
| `upload_model.py` | Upload model to HF Hub | `python upload_model.py --model_path model.h5` |
| `test_inference.py` | Test model inference | `python test_inference.py --image test.jpg` |

### Model Repository

The model is publicly available at:
- **ü§ó Hugging Face Hub**: https://huggingface.co/CemRoot/deepfake-detection-model
- **Repository Type**: Model
- **Access**: Public
- **License**: MIT

---

## Project Structure

```
deepfake-detection-streamlit/
‚îÇ
‚îú‚îÄ‚îÄ app.py                          # Main application entry point (imports from src/)
‚îÇ
‚îú‚îÄ‚îÄ src/                            # Source code directory
‚îÇ   ‚îú‚îÄ‚îÄ __init__.py                 # Package initialization
‚îÇ   ‚îÇ
‚îÇ   ‚îú‚îÄ‚îÄ app.py                      # Main Streamlit application logic
‚îÇ   ‚îÇ
‚îÇ   ‚îú‚îÄ‚îÄ model/                      # Model architecture and loading
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ architecture.py         # EfficientNetB7 + Attention model definition
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ loader.py               # Model loading from Hugging Face Hub
‚îÇ   ‚îÇ
‚îÇ   ‚îú‚îÄ‚îÄ preprocessing/              # Image preprocessing pipeline
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ image_processor.py      # Image preprocessing functions
‚îÇ   ‚îÇ
‚îÇ   ‚îî‚îÄ‚îÄ ui/                         # User interface components
‚îÇ       ‚îú‚îÄ‚îÄ __init__.py
‚îÇ       ‚îú‚îÄ‚îÄ styles.py               # Custom CSS styling
‚îÇ       ‚îî‚îÄ‚îÄ components.py           # Reusable UI components (header, footer, sidebar)
‚îÇ
‚îú‚îÄ‚îÄ huggingface/                    # Hugging Face Hub integration tools
‚îÇ   ‚îú‚îÄ‚îÄ README.md                   # Quick start guide
‚îÇ   ‚îú‚îÄ‚îÄ HUGGINGFACE_GUIDE.md        # Detailed upload/management guide (Turkish)
‚îÇ   ‚îú‚îÄ‚îÄ upload_model.py             # Script to upload model to HF Hub
‚îÇ   ‚îú‚îÄ‚îÄ test_inference.py           # Test and inference script
‚îÇ   ‚îú‚îÄ‚îÄ README_MODEL.md             # Model card template for HF Hub
‚îÇ   ‚îú‚îÄ‚îÄ config.json                 # Model configuration and metadata
‚îÇ   ‚îú‚îÄ‚îÄ requirements.txt            # Dependencies for HF integration
‚îÇ   ‚îî‚îÄ‚îÄ .gitattributes              # Git LFS configuration
‚îÇ
‚îú‚îÄ‚îÄ docs/                           # Documentation directory
‚îÇ   ‚îú‚îÄ‚îÄ DEPLOYMENT.md               # Deployment instructions
‚îÇ   ‚îú‚îÄ‚îÄ QUICKSTART.md               # Quick start guide
‚îÇ   ‚îú‚îÄ‚îÄ PREPROCESSING_ANALYSIS.md  # Preprocessing method analysis
‚îÇ   ‚îî‚îÄ‚îÄ README_HF.md                # Hugging Face README
‚îÇ
‚îú‚îÄ‚îÄ .streamlit/                     # Streamlit configuration
‚îÇ   ‚îî‚îÄ‚îÄ config.toml                 # Application settings
‚îÇ
‚îú‚îÄ‚îÄ requirements.txt                # Python package dependencies
‚îÇ   ‚îú‚îÄ‚îÄ TensorFlow 2.15
‚îÇ   ‚îú‚îÄ‚îÄ Streamlit framework
‚îÇ   ‚îú‚îÄ‚îÄ Image processing libraries
‚îÇ   ‚îî‚îÄ‚îÄ Support utilities
‚îÇ
‚îú‚îÄ‚îÄ packages.txt                    # System-level package dependencies
‚îÇ
‚îú‚îÄ‚îÄ Dockerfile                      # Docker containerization
‚îÇ
‚îú‚îÄ‚îÄ README.md                       # Comprehensive documentation (this file)
‚îÇ
‚îú‚îÄ‚îÄ .gitignore                      # Version control exclusions
‚îÇ
‚îî‚îÄ‚îÄ LICENSE                         # MIT License agreement
```

### Code Organization

The application follows a modular architecture with clear separation of concerns:

**Core Modules:**

1. **Model Architecture (`src/model/architecture.py`)**:
   - `build_effatt_model()`: EfficientNetB7 + Attention model construction
   - `attention_block()`: Custom attention mechanism implementation

2. **Model Loading (`src/model/loader.py`)**:
   - `load_model()`: Hugging Face Hub integration with caching
   - Automatic model download and weight loading

3. **Image Preprocessing (`src/preprocessing/image_processor.py`)**:
   - `preprocess_image()`: Multiple preprocessing strategies
   - Support for training-match, simple normalization, and EfficientNet preprocessing

4. **User Interface (`src/ui/`)**:
   - `styles.py`: Professional CSS styling for academic presentation
   - `components.py`: Reusable UI components (header, sidebar, footer)
   - Modern, responsive design optimized for conference demonstrations

5. **Main Application (`src/app.py`)**:
   - Streamlit application logic
   - Inference pipeline and result visualization
   - Integration of all modules

**Design Benefits:**
- **Modularity**: Easy to maintain and extend individual components
- **Reusability**: UI components and preprocessing functions can be reused
- **Testability**: Each module can be tested independently
- **Scalability**: Simple to add new features or preprocessing methods
- **Clarity**: Clear separation between model, preprocessing, and UI logic

## Usage Guide

### Basic Operation Workflow

#### 1. Image Upload
- Click the **"Browse files"** button or use drag-and-drop functionality
- Supported formats: JPG, JPEG, PNG
- Recommended: High-resolution images for optimal detection accuracy

#### 2. Configuration (Optional)
Access the sidebar settings to configure:
- **Preprocessing Method**: Select appropriate preprocessing strategy
  - **Training Match** (Recommended): Exact replication of training preprocessing
  - **Simple Normalization**: Standard [0,1] normalization
  - **EfficientNet ImageNet**: ImageNet-specific preprocessing
- **Debug Mode**: Enable detailed diagnostic information

#### 3. Analysis Execution
- Click **"Analyze Image"** button
- Processing time: Approximately 2-3 seconds on standard CPU
- Real-time progress indicator displayed during inference

#### 4. Result Interpretation
The system provides:
- **Classification Result**: FAKE (AI-Generated) or AUTHENTIC (Genuine)
- **Confidence Scores**: Probability distribution for both classes
- **Visual Indicators**: Color-coded result presentation
- **Debug Information** (if enabled): Technical diagnostics

### Example Output

#### Synthetic Image Detection
```
üö® DETECTION RESULT: FAKE (AI-Generated)

The input image exhibits characteristics consistent with synthetic
generation by artificial intelligence systems.

üìà Classification Confidence Scores:
üö® Synthetic (AI-Generated): 87.45%
‚úÖ Authentic (Genuine): 12.55%

Classification Output: Fake (Confidence: 87.45%)
```

#### Authentic Image Detection
```
‚úÖ DETECTION RESULT: AUTHENTIC

The input image exhibits characteristics consistent with genuine
photographic content.

üìà Classification Confidence Scores:
üö® Synthetic (AI-Generated): 15.32%
‚úÖ Authentic (Genuine): 84.68%

Classification Output: Real (Confidence: 84.68%)
```

## Technical Configuration

### Model Repository

The trained neural network weights are hosted on Hugging Face Model Hub, ensuring reliable access and version control:

- **Repository ID**: `CemRoot/deepfake-detection-model`
- **Model Filename**: `best_model_effatt.h5`
- **File Size**: Approximately 780 MB
- **Format**: Keras H5 (TensorFlow 2.15/Keras compatible)
- **Access**: Public repository with automatic download
- **Caching**: Local caching implemented for efficient repeated access

### Model Download Behavior

The system automatically handles model acquisition:
1. On first application launch, the model is downloaded from Hugging Face
2. Downloaded weights are cached locally in user's home directory
3. Subsequent launches utilize cached model, eliminating download time
4. No manual configuration required

### Input Image Specifications

**Supported Formats**
- JPEG (.jpg, .jpeg)
- PNG (.png)

**Preprocessing Pipeline**
- Input images of any resolution are automatically resized to 128√ó128 pixels
- Color mode: RGB (3-channel color images)
- Grayscale images are automatically converted to RGB
- RGBA images (with alpha channel) are converted to RGB

**Recommended Characteristics**
- Resolution: 256√ó256 pixels or higher for optimal quality
- File size: < 10 MB
- Minimal compression artifacts
- Adequate illumination and focus

## Performance Metrics

### Computational Performance

**Inference Latency**
- **CPU-based inference**: 2-3 seconds per image (tested on Intel i5-8250U)
- **GPU-accelerated inference**: < 1 second per image (NVIDIA GTX 1060 or equivalent)
- **Batch processing**: Supports batch inference for multiple images

**Resource Utilization**
- **Memory consumption**: ~2 GB RAM (including model weights and runtime overhead)
- **Storage requirements**: ~1.5 GB (model weights + dependencies)
- **CPU usage**: Single-threaded inference (parallel processing supported for batches)

### Classification Performance

**Note**: Detailed accuracy metrics, precision, recall, F1-scores, and ROC curves are available in the full conference paper presented at AICS 2025.

**Generalization Characteristics**
- High accuracy on held-out test datasets
- Robust performance across multiple generative model families
- Effective detection of both GAN-generated and diffusion-generated imagery
- Maintained performance under common image transformations

## AICS 2025 Conference Demo

### Demo Information

This system is being presented as a demonstration at:

**33rd Irish Conference on Artificial Intelligence and Cognitive Science (AICS 2025)**

**Demonstration Features**
- Live image analysis with real-time results
- Interactive preprocessing method comparison
- Debug mode for technical audience engagement
- Visual confidence score presentation
- Support for audience-submitted test images

**Demo Setup Recommendations**
1. Use the cloud-deployed version for reliability and accessibility
2. Enable **Training Match** preprocessing for optimal accuracy
3. Prepare diverse test images (both authentic and synthetic)
4. Utilize debug mode when presenting to technical audiences
5. Have example outputs ready for quick reference

### Conference Presentation Highlights

**Research Contributions**
- Novel attention mechanism design for deepfake detection
- Comprehensive evaluation across multiple generative model families
- Practical deployment as accessible web application
- Robust preprocessing pipeline ensuring training-inference consistency

**Technical Demonstration Points**
- Real-time inference capabilities
- High classification accuracy with interpretable confidence scores
- Extensibility for future generative model architectures
- Practical applicability for media authentication scenarios

---

## Academic Citation

If you use this system in your research, academic work, or build upon this implementation, please cite:

```bibtex
@inproceedings{koyluoglu2025deepfake,
  title={Advanced DeepFake Detection using EfficientNetB7 with Attention Mechanism},
  author={Koyluoglu, Emin Cem},
  booktitle={Proceedings of the 33rd Irish Conference on Artificial Intelligence and Cognitive Science (AICS 2025)},
  year={2025},
  organization={AICS},
  note={Demo presentation}
}
```

**Alternative Citation Format (APA)**
```
Koyluoglu, E. C. (2025). Advanced DeepFake Detection using EfficientNetB7
with Attention Mechanism. Presented at the 33rd Irish Conference on
Artificial Intelligence and Cognitive Science (AICS 2025).
```

---

## License

This project is licensed under the **MIT License**.

### License Summary
- ‚úÖ Commercial use permitted
- ‚úÖ Modification permitted
- ‚úÖ Distribution permitted
- ‚úÖ Private use permitted
- ‚ö†Ô∏è Liability and warranty disclaimers apply

See the [LICENSE](LICENSE) file for complete legal terms.

---

## Contributing

Contributions to this project are welcome and encouraged. Areas for potential contribution include:

**Model Improvements**
- Training on additional generative model architectures
- Hyperparameter optimization experiments
- Alternative attention mechanism designs
- Multi-class classification extensions

**Application Enhancements**
- Batch processing capabilities
- Additional preprocessing methods
- Performance optimization
- Extended visualization options

**Documentation**
- Additional usage examples
- Tutorial notebooks
- API documentation
- Deployment guides

**Contribution Process**
1. Fork the repository
2. Create a feature branch (`git checkout -b feature/YourFeature`)
3. Commit your changes (`git commit -m 'Add YourFeature'`)
4. Push to the branch (`git push origin feature/YourFeature`)
5. Open a Pull Request with detailed description

---

## Author

**Emin Cem Koyluoglu**

- **GitHub**: [@CemRoot](https://github.com/CemRoot)
- **Website**: [cemkoyluoglu.codes](https://cemkoyluoglu.codes/)
- **Conference**: AICS 2025 (33rd Irish Conference on AI and Cognitive Science)
- **Project**: DeepFake Detection System

**Research Interests**: Deep Learning, Computer Vision, Generative AI, Media Forensics

---

## Acknowledgments

This work has been developed with support from various open-source communities and platforms:

**Infrastructure**
- **Hugging Face**: Model hosting and distribution infrastructure
- **Streamlit**: Web application framework and cloud deployment platform
- **GitHub**: Version control and collaborative development

**Technical Foundations**
- **TensorFlow/Keras Development Team**: Deep learning framework
- **EfficientNet Authors**: Base architecture (Tan & Le, 2019)
- **Open-source Computer Vision Community**: Image processing tools

**Conference**
- **AICS 2025 Organizing Committee**: Opportunity to present this research
- **Conference Reviewers**: Valuable feedback and suggestions

**Special Thanks**
- Open-source contributors who maintain the libraries used in this project
- Research community advancing deepfake detection methodologies
- Early testers who provided feedback on the web interface

---

## Contact and Support

### For Research Inquiries
- Email: Available on [GitHub profile](https://github.com/CemRoot)
- Website: [cemkoyluoglu.codes](https://cemkoyluoglu.codes/)

### For Technical Issues
1. **GitHub Issues**: [Create an issue](https://github.com/CemRoot/deepfake-detection-streamlit/issues) for bug reports or feature requests
2. **Pull Requests**: Submit improvements or bug fixes directly
3. **Discussions**: Use GitHub Discussions for questions and community interaction

### For General Questions
- **Streamlit Community**: [Streamlit Forum](https://discuss.streamlit.io/) for deployment and UI questions
- **Documentation**: Refer to this README for comprehensive usage information

---

## Future Work

Potential directions for system enhancement:

**Model Architecture**
- Exploration of Vision Transformer (ViT) architectures
- Ensemble methods combining multiple detection approaches
- Lightweight models for mobile deployment
- Explainable AI techniques for decision interpretation

**Dataset Expansion**
- Incorporation of emerging generative models
- Cross-dataset generalization studies
- Adversarial robustness evaluation
- Video deepfake detection extension

**Application Development**
- Browser extension for in-situ detection
- Mobile application deployment
- API service for integration with other platforms
- Real-time video stream analysis

---

<div align="center">

## System Information

**Presented at AICS 2025**
33rd Irish Conference on Artificial Intelligence and Cognitive Science

**Developed by Emin Cem Koyluoglu**

‚ö° **Powered by**: Streamlit Cloud | Hugging Face | TensorFlow
üî¨ **Research Area**: AI-Generated Image Detection
üéØ **Application**: Media Authentication & Digital Forensics

---

**License**: MIT | **Language**: Python 3.10+ | **Framework**: TensorFlow 2.15

Built with dedication for advancing media integrity research

[GitHub Repository](https://github.com/CemRoot/deepfake-detection-streamlit) | [Live Demo](https://your-app-url.streamlit.app) | [Author Website](https://cemkoyluoglu.codes/)

</div>

